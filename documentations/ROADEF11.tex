\documentclass[slidetop,11pt]{beamer}

% Ces deux lignes à décommenter pour sortir
% le texte en classe article
% \documentclass[class=article,11pt,a4paper]{beamer}
% \usepackage{beamerbasearticle}

% Packages pour les français
%
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[frenchb]{babel}
% pour un pdf lisible à l'écran si on ne dispose pas
% des fontes cmsuper ou lmodern
%\usepackage{lmodern}
\usepackage{aeguill}

% Pour afficher le pdf en plein ecran
% (commenté pour imprimer les transparents et pour les tests)
%\hypersetup{pdfpagemode=FullScreen}

% ------------------------------------------------
%-----------   styles pour beamer   --------------

% Supprimer les icones de navigation (pour les transparents)
\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}
{
\hfill\insertframenumber/\inserttotalframenumber~~~
}

% Mettre les icones de navigation en mode vertical (pour projection)
%\setbeamertemplate{navigation symbols}[vertical]

% ------------ Choix des thèmes ------------------

%\usetheme{Boadilla}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Marburg}
%\usetheme{Warsaw}
\usetheme{Frankfurt}

%------------ fin style beamer -------------------

\title{Un algorithme d'optimisation à haute dimension pour la fouille de données}
\subtitle{Méthode et application en onco-pharmacogénomique}
\author{\underline{Vincent Gardeux}, René Natowicz, Rachid Chelouah, Roman Rouzier, Antônio Braga Padua, Patrick Siarry}
\institute{L@ris, EISTI\\LiSSi, Université Paris-Est\\Université Paris-Est, ESIEE-Paris\\Hôpital Tenon}
\date{ROADEF 2011 - 2,3 et 4 Mars 2011}

% ------------------------------------------------
% -------------   Début document   ---------------
% ------------------------------------------------
\begin{document}
%--------- Écriture de la page de titre ----------
% avec la commande frame simplifiée
\frame{\titlepage}
%
%------------------ Sommaire ---------------
\begin{frame}{Sommaire}
  \small \tableofcontents[hideallsubsections]
\end{frame}
%
%***************************************
%******     Les slides          *******
%***************************************

\section{Introduction}

\subsection{Fouille de données}

\begin{frame}[fragile]
\frametitle{Fouille de données}
La fouille de données ou \emph{Data Mining} permet d'extraire la connaissance à partir d'un ensemble de données.\\
On peut la décomposer en plusieurs phases :
\begin{itemize}
\item \underline{\emph{Feature Selection} }: Phase de sélection des variables les plus intéressantes (les variables apportant le plus d'information pour la future classification).
\item \underline{Classification }:
    \begin{itemize}
    \item Apprentissage : On calcule à partir des variables sélectionnées, les règles qui nous permettront de dire à quelle classe appartient un nouvel individu
    \item Validation : On estime les performances de notre classifieur
    \end{itemize}
\end{itemize}
\end{frame}

\subsection{Puce à ADN}

\begin{frame}
    \frametitle{Puce à ADN}
\begin{itemize}
\item Données fournies par des puces à ADN
\item Fournissent le niveau d'expression d'un ensemble de gènes pour un patient donné
\begin{figure}
\includegraphics[height=150px]{DNAMicroarray.jpg}
\end{figure}
\end{itemize}
\end{frame}

\section{Description du problème}

\subsection{Données}

\begin{frame}
    \frametitle{Données}
\begin{itemize}
\item Patientes atteintes du cancer du sein
\item 2 sources de données utilisant la même puce à ADN (Affymetrix U133A) de 22283 gènes
\begin{itemize}
\item Houston : 82 Patients
\item Villejuif : 51 Patients
\end{itemize}
\item 2 classes de patientes : pour chaque patiente, on sait si elle a été répondeuse (PCR : pathologic complete response) ou non (No-PCR) au traitement chimiothérapique
\item Données importantes : $\simeq 20 000$ gènes pour très peu de cas : $\simeq 100$ patients
\item But : Prédire la classe d'un nouveau patient en fonction des niveaux d'expression de ses gènes.
\end{itemize}
\end{frame}


\subsection{Méthodes}

\begin{frame}
    \frametitle{Méthodes}
\begin{itemize}
\item On utilisera des procédés pour augmenter virtuellement le nombre de cas d'études (cross-validation, bootstrapping, ...)
\item On prend le parti de se focaliser sur l'étape de \emph{Feature Selection}
\item Après sélection d'un sous-ensemble de gènes par notre méthode, on utilisera des classifieurs existants pour la création du modèle de prédiction : LDA, DLDA, SVM, ... (disponibles dans Matlab, R, ...)
\end{itemize}
\end{frame}

\subsection{Normalisation des données}

\begin{frame}
    \frametitle{Normalisation des données}
\begin{block}{\textbf{Normalisation Logarithmique}}
    \begin{itemize}
        \item Normalisation nécessaire quand les variables peuvent prendre des valeurs sur plusieurs décades (typiquement > 5 décades)
        \item Utile pour ramener les données sur une échelle commune
    \end{itemize}
\end{block}

\begin{block}{\textbf{Méthode}}
    But : Normaliser la variation des probes pour toutes les patientes.\\
    Soit $x = \{x_1,...,x_i,...,x_n\}$ le vecteur à normaliser, et $x_{max}$ et $x_{min}$ ses extrema. Chaque composant $\dot{x}_i$ du vecteur normalisé $\dot{x}$ est calculé ainsi :
    \begin{equation}
    \dot{x}_i = \frac{log(x_i) - log(x_{min})}{log(x_{max}) - log(x_{min})}
    \end{equation}
avec $log$ l'opérateur logarithmique décimal
\end{block}

\end{frame}

\section{Méthode}

\subsection{Algorithme de Feature Selection - Etat de l'art}

\begin{frame}
    \frametitle{Algorithme de \emph{Feature Selection} - Etat de l'art}
\begin{itemize}
\item La plupart des méthodes de \emph{Feature Selection} actuelles se basent sur des méthodes statistiques pour sélectionner les variables les moins redondantes et apportant le maximum d'information (Student t-test, R-squared, ...)\\
\item Il existe également des techniques d'optimisation (LASSO, algorithmes génétiques, ...), mais la plupart utilisent tout d'abord un test statistique afin de limiter la dimension de l'espace de recherche.
\end{itemize}
\end{frame}

\subsection{Algorithme de Feature Selection - But}

\begin{frame}
    \frametitle{Algorithme de \emph{Feature Selection} - But}
\begin{itemize}
\item Sélectionner le sous-ensemble de gènes le plus représentatif :
    \begin{itemize}
    \item Maximiser la distance entre les 2 classes de patientes (distance interclasse)
    \item Minimiser la taille du sous-ensemble de gènes sélectionnés, afin d'éviter le sur-apprentissage des données (\emph{data overfitting})
    \end{itemize}
    $\Rightarrow$ Optimisation bi-objectif\\
    $\Rightarrow$ Les 2 objectifs sont contradictoires
\item Fonction objectif comme combinaison linéaire convexe de ces 2 objectifs, pondérés par un paramètre $w$ ($w \in [0,1]$)
\begin{equation}
    F_w(s) = w * d(G(s),G'(s)) + (1 - w) * (1 - |s|)
\end{equation}
\end{itemize}
\end{frame}

\subsection{Algorithme de Feature Selection - Définitions}

\begin{frame}
    \frametitle{Algorithme de \emph{Feature Selection} - Définitions}
  \begin{block}{\textbf{Distance interclasse : }}
    Soient G(s) et G'(s) les deux vecteurs centres de gravité des deux classes répondeuses et non répondeuses pour le sous-ensemble de sondes s. La distance interclasse est la distance euclidienne entre les deux centres de gravité G(s) et G'(s)
  \end{block}
\begin{itemize}
\item La fonction bi-objectif est à optimiser dans l'ensemble des sous-ensembles de sondes (ensemble de taille $2^{22283}$)
\item Une solution est un vecteur binaire de dimension 22283
\end{itemize}
\emph{Ex :}\\
$x = \{0,1,0,0,0,1,0,...,1\}$\\
$\Rightarrow$ Un 1 en position $i$, signifie que le $i^{eme}$ gène est sélectionné
\end{frame}

\subsection{Algorithme de Feature Selection - Méthode}

\begin{frame}
    \frametitle{Algorithme de \emph{Feature Selection} - Méthode}
\begin{itemize}
\item Problème d'optimisation haute dimension : nous nous sommes basés sur l'algorithme EUS développé pour les problèmes d'optimisation à grande dimension
\begin{block}{Principe de EUS}
    \begin{itemize}
        \item Méthode de relaxation pour décomposer la fonction objectif dimension par dimension
        \item Line Search pour optimiser la fonction objectif sur chaque dimension
        \item Procédure de redémarrage pour explorer l'espace de recherche
    \end{itemize}
\end{block}
\item Discrétisation de l'algorithme EUS (DEUS)
$\Rightarrow$ Application sur un vecteur binaire
\item Algorithme résultant : suite de minimisations locales
\begin{itemize}
\item Gène retiré ou ajouté au sous-ensemble si cela augmente la valeur de la fonction objectif
\item On recommence jusqu'à stabilisation
\end{itemize}
\end{itemize}
\end{frame}


\subsection{Algorithme de Feature Selection - DEUS pseudo code}
\begin{frame}
    \frametitle{Algorithme de \emph{Feature Selection} - DEUS pseudo code}
    \begin{figure}[bhtp!]
    \fbox{
        \begin{minipage}{\textwidth}
        \textbf{Procedure DEUS}\\
        \textbf{begin}\\
    	\hspace*{.5cm} Initialiser le vecteur binaire $S$ aléatoirement\\
    	\hspace*{.5cm} \textbf{do}\\
    	\hspace*{1cm} $v = f(S)$ (avec $f$ la fonction objectif)\\
        \hspace*{1cm} \textbf{for} $i = 1$ \textbf{to} $n$\\
        \hspace*{1.5cm} permuter la $i^{eme}$ valeur binaire de $S$\\
        \hspace*{1.5cm} $v' = f(S)$\\
        \hspace*{1.5cm} \textbf{if}(v' < v) \textbf{then} permuter à nouveau la $i^{eme}$ valeur de $S$\\
        \hspace*{1cm} \textbf{end for}\\
        \hspace*{.5cm} \textbf{while} aucune meilleure solution n'est trouvée durant une itération complète\\
        \textbf{end}\\
        \end{minipage}
    }
    \end{figure}
\end{frame}

\section{Résultats}

\subsection{Protocole Expérimental}

\begin{frame}
    \frametitle{Protocole Expérimental}

\begin{itemize}
\item Comparaison avec 2 méthodes récentes :
\begin{itemize}
\item Hess 2006 (t-test + DLDA)
\item Natowicz 2008 (BI Majorité 30 + DLDA)
\end{itemize}
\end{itemize}

\begin{itemize}
    \item Nous avons suivi le protocole utilisé par Hess et Natowicz :
    \begin{itemize}
    \item Données Houston : Population d'apprentissage (pour la sélection des gènes et pour la méthode de classification)
    \item Données Villejuif : Population de validation (pour la méthode de classification)
    \end{itemize}
    \item Chaque population contient 1/3 de patient répondant au traitement et 2/3 non répondant
\end{itemize}

\begin{itemize}
\item Méthode de classification utilisée : DLDA (\emph{Diagonal Linear Discriminant Analysis})
\end{itemize}


\end{frame}

\subsection{Résultats comparatifs}

\begin{frame}
    \frametitle{Résultats comparatifs}
    \begin{table}[ht]
    \tiny
    \begin{center}
    \begin{tabular}{c c c c c c}
    \hline
    & DEUS DLDA 31 & DEUS DLDA 11 & t-test DLDA 31 & BI Majorité 30\\
    Distance interclasse & 5175,45 & \textbf{3670,45} & 1383,30 & 3210,28 \\
    \hline
    Précision&0,863&\textbf{0,882}&0,765&0,863\\
    Sensibilité&0,846&\textbf{0,923}&0,923&0,923\\
    Spécificité&0,868&\textbf{0,868}&0,711&0,842\\
    VPP&0,688&\textbf{0,706}&0,522&0,667\\
    VPN &0,943&\textbf{0,971}&0,964&0,970\\
    \end{tabular}
    \end{center}
    \end{table}
    \begin{itemize}
    \item Sensibilité : Probabilité d'être classifiée répondeuse si elle l'est
    \item Spécificité : Probabilité d'être classifiée non répondeuse si elle l'est
    \item VPP :  Valeur Prédictive Positive = probabilité d'être répondeuse si classifiée ainsi
    \item VPN : Valeur Prédictive Négative = probabilité d'être non répondeuse si classifiée ainsi
    \end{itemize}
\end{frame}

\subsection{Gènes sélectionnés}

\begin{frame}
    \frametitle{Gènes sélectionnés}
    Les 11 gènes sélectionnés par DEUS sont les suivants :
    \begin{table}[ht]
    \begin{center}
    \begin{tabular}{| c | c | c | c |}
    \hline
    Sonde Affymetrix & Nom du Gène & Hess & Natowicz\\
    \hline
    203929\_s\_at&\textbf{MAPT}&X&\\
    204825\_at&MELK&X&X\\
    204913\_s\_at&SOX11&&\\
    205225\_at&\textbf{ESR1}&&\\
    205354\_at&GAMT&X&\\
    205548\_s\_at&\textbf{BTG3}&X&X\\
    209173\_at&AGR2&&\\
    212956\_at&AI348094&&\\
    213134\_x\_at&\textbf{BTG3}&X&X\\
    218211\_s\_at&MLPH&&\\
    219051\_x\_at&METRN&X&X\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
\end{frame}

\section{Conclusion}

\subsection{Conclusion}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
    \item Nos résultats surpassent les autres, mais le résultat le plus intéressant est qu'ils sont obtenus avec 3 fois moins de probes (11 au lieu de 30)
    \item Rapidité de convergence : quelques itérations suffisent
    \item Solution stable :
    \begin{itemize}
    \item Ne dépend pas de la solution initiale
    \item Ne dépend pas de l'ordre dans lequel les gènes sont examinés
    \item A chaque redémarrage, converge vers la même solution pour un $w$ donné
    \end{itemize}
	\item La Sensibilité est élevée => important pour les biologistes et éventuellement pour la mise en place d'une routine clinique
	\item Une Cross-validation 3-fold sur l'ensemble des 133 patientes obtient d'excellents résultats => Accentue la robustesse de la méthode
    \end{itemize}
\end{frame}

\subsection{Travaux en cours}

\begin{frame}
    \frametitle{Travaux en cours}
    \begin{itemize}
	\item La Méthode Lasso "regularisation path via coordinate descent" (Trevor Hastie \& al.) retourne des prédicteurs ayant plus de sondes et 		  	dont les performances sont inférieures.
	\item Fonction objectif intégrant inertie interclasse et inertie intraclasse
	\item Fonction objectif intégrant la corrélation entre les gènes
    \item Application à d'autres domaines d'application
    \end{itemize}
\end{frame}

\begin{frame}
 	\begin{block}{ }
		\center Merci pour votre attention\\
		\center Des questions ?
	\end{block}
	\begin{center}
		Vincent Gardeux\\
		Enseignant-chercheur EISTI\\
		Doctorant de l'Université Paris-Est Créteil
	\end{center}

	\begin{center}
		Davantage d'informations à propos des recherches en cours :\\
		\underline{http://gardeux-vincent.eu}
	\end{center}
\end{frame}

\end{document}
